{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "The first step before running the clustering algorithm is to prepare the training and the testing data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n",
    "\n",
    "fileNameTrain = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\train.json\\\\train.json\"\n",
    "train_df = pd.read_json(fileNameTrain)\n",
    "\n",
    "fileNameTest = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\test.json\\\\test.json\"\n",
    "test_df = pd.read_json(fileNameTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will extract the key words from the description variable - by key words, we define words that are in the description of the unit but are not stop words as defined by the ntlk.corpus package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "train_df['description_key_words'] = pd.Series(description_key_words_ls, index=train_df.index)\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "test_df['description_key_words'] = pd.Series(description_key_words_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two numeric variables which describe the number of features and number of key words in the description section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['num_features'] = train_df.features.apply(len)\n",
    "train_df['num_key_words_description'] = train_df.description_key_words.apply(len)\n",
    "\n",
    "test_df['num_features'] = test_df.features.apply(len)\n",
    "test_df['num_key_words_description'] = test_df.description_key_words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Created variable, we will extract into new variables the exact data when the listing was created, the day of year, week of year, weekday and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"date\"]= train_df[\"created\"].dt.date\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "train_df[\"weekofyear\"] = train_df[\"created\"].dt.weekofyear\n",
    "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
    "train_df[\"hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"date\"]= test_df[\"created\"].dt.date\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "test_df[\"weekofyear\"] = test_df[\"created\"].dt.weekofyear\n",
    "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
    "test_df[\"hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the number of photos of each listing as a new variable in the training and testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step is to create two variables which describe the price per bathroom and price per bedroom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"price_per_bathroom\"] = train_df[\"price\"]/(train_df[\"bathrooms\"] + 1)\n",
    "train_df[\"price_per_bedroom\"] = train_df[\"price\"]/(train_df[\"bedrooms\"] + 1)\n",
    "\n",
    "test_df[\"price_per_bathroom\"] = test_df[\"price\"]/(test_df[\"bathrooms\"] + 1)\n",
    "test_df[\"price_per_bedroom\"] = test_df[\"price\"]/(test_df[\"bedrooms\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the building id as a unique identifier of the building and the building itself affects the interest level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['building_id'].values) + list(test_df['building_id'].values))\n",
    "train_df['building_id'] = lbl.transform(list(train_df['building_id'].values))\n",
    "\n",
    "test_df['building_id'] = lbl.transform(list(test_df['building_id'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count how many rental units per building are available for rent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "building_id_group_by = train_df[[\"street_address\", \"building_id\"]].groupby([\"building_id\"]).count()\n",
    "building_id_group_by = building_id_group_by.add_suffix('_count').reset_index()\n",
    "\n",
    "train_df = train_df.merge(building_id_group_by, left_on=['building_id'], right_on=['building_id'], how='inner')\n",
    "test_df = test_df.merge(building_id_group_by, left_on=['building_id'], right_on=['building_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some buildings are more often seen in rental ads than others. We will introduce a number of few variables which describe how often the building is been seen in a rental post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step in the data preparation, we will explore furthere the description of the rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "\n",
    "sf = shapefile.Reader(\"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\ZillowNeighborhoods-NY\\\\ZillowNeighborhoods-NY.shp\")\n",
    "\n",
    "shapes = sf.shapes()\n",
    "records = sf.records()\n",
    "\n",
    "towns_values = [records[i][2] for i in range(len(records))]\n",
    "neighb_values = [records[i][3] for i in range(len(records))]\n",
    "west_values = [shapes[i].bbox[0] for i in range(len(records))]\n",
    "south_values = [shapes[i].bbox[1] for i in range(len(records))]\n",
    "east_values = [shapes[i].bbox[2] for i in range(len(records))]\n",
    "north_values = [shapes[i].bbox[3] for i in range(len(records))]\n",
    "\n",
    "west, south, east, north = -74.02, 40.64, -73.85, 40.86\n",
    "\n",
    "neighbourhood_pd = pd.DataFrame({'Town' : towns_values,\n",
    "                                 'Neighbourhood' : neighb_values,\n",
    "                                 'West' : west_values,\n",
    "                                 'South' : south_values,\n",
    "                                 'East' : east_values,\n",
    "                                 'North' : north_values})\n",
    "\n",
    "neighbourhood_pd = neighbourhood_pd[neighbourhood_pd.Town == \"New York\"]\n",
    "neighbourhood_pd = neighbourhood_pd.ix[(neighbourhood_pd.West >= west) & \n",
    "                                     (neighbourhood_pd.East <= east) & \n",
    "                                     (neighbourhood_pd.South >= south) & \n",
    "                                     (neighbourhood_pd.North <= north)]\n",
    "\n",
    "neighbourhood_sorted_pd = neighbourhood_pd.sort_values(['West'])\n",
    "\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, train_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<train_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>train_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "train_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=train_df.index)\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, test_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<test_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>test_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "test_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_back_up = train_df\n",
    "test_df_back_up = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df_back_up\n",
    "test_df = test_df_back_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df = train_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "#test_df = test_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "\n",
    "train_group_by = train_df[[\"bedrooms\", \"neighbourhood\", \"price\"]].groupby([\"bedrooms\", \"neighbourhood\"]).median()\n",
    "train_group_by = train_group_by.add_suffix('_median').reset_index()\n",
    "\n",
    "train_df = train_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='outer')\n",
    "test_df = test_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='left')\n",
    "\n",
    "train_df[\"price_diff\"] = train_df[\"price\"] - train_df[\"price_median\"]\n",
    "test_df[\"price_diff\"] = test_df[\"price\"] - test_df[\"price_median\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a number of new variables which describe how often certain building is seen in rental posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_count = train_df['building_id'].value_counts()\n",
    "\n",
    "train_df['top_10_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_df['top_25_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_df['top_5_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_df['top_50_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_df['top_1_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_df['top_2_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_df['top_15_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_df['top_20_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_df['top_30_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n",
    "\n",
    "test_df['top_10_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "test_df['top_25_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "test_df['top_5_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "test_df['top_50_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "test_df['top_1_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "test_df['top_2_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "test_df['top_15_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "test_df['top_20_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "test_df['top_30_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the feature variable to crate a number of new variables based on the features the customers may be looking for. The idea comes from https://www.kaggle.com/jamestollefson/two-sigma-connect-rental-listing-inquiries/feature-engineering-train-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def newfeat(name, df, series):\n",
    "    \"\"\"Create a Series for my feature building loop to fill\"\"\"\n",
    "    feature = pd.Series(0, df.index, name=name)\n",
    "    \"\"\"Now populate the new Series with numeric values\"\"\"\n",
    "    for row, word in enumerate(series):\n",
    "        if name in word:\n",
    "            feature.iloc[row] = 1\n",
    "    df[name] = feature\n",
    "    return(df)\n",
    "   \n",
    "train_df = newfeat('Elevator', train_df, train_df.features)\n",
    "train_df = newfeat('Dogs Allowed', train_df, train_df.features)\n",
    "train_df = newfeat('Cats Allowed', train_df, train_df.features)\n",
    "train_df = newfeat('Hardwood Floors', train_df, train_df.features)\n",
    "train_df = newfeat('Swimming Pool', train_df, train_df.features)\n",
    "train_df = newfeat('Doorman', train_df, train_df.features)\n",
    "train_df = newfeat('Laundry in Unit', train_df, train_df.features)\n",
    "train_df = newfeat('Fitness center', train_df, train_df.features)\n",
    "train_df = newfeat('gym', train_df, train_df.features)\n",
    "train_df = newfeat('Pre-War', train_df, train_df.features)\n",
    "train_df = newfeat('private-balcony', train_df, train_df.features)\n",
    "train_df = newfeat('balcony', train_df, train_df.features)\n",
    "train_df = newfeat('Laundry in Building', train_df, train_df.features)\n",
    "\n",
    "\n",
    "test_df = newfeat('Elevator', test_df, test_df.features)\n",
    "test_df = newfeat('Dogs Allowed', test_df, test_df.features)\n",
    "test_df = newfeat('Cats Allowed', test_df, test_df.features)\n",
    "test_df = newfeat('Hardwood Floors', test_df, test_df.features)\n",
    "test_df = newfeat('Swimming Pool', test_df, test_df.features)\n",
    "test_df = newfeat('Doorman', test_df, test_df.features)\n",
    "test_df = newfeat('Laundry in Unit', test_df, test_df.features)\n",
    "test_df = newfeat('Fitness center', test_df, test_df.features)\n",
    "test_df = newfeat('gym', test_df, test_df.features)\n",
    "test_df = newfeat('Pre-War', test_df, test_df.features)\n",
    "test_df = newfeat('private-balcony', test_df, test_df.features)\n",
    "test_df = newfeat('balcony', test_df, test_df.features)\n",
    "test_df = newfeat('Laundry in Building', test_df, test_df.features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the listing_id variable to create the listing difference variable which describes how far away from the minimum listing id is a given rental post. As we already discovered in our descriptive analysis, the listing difference affects the interest level of a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dayofyear_group_by = train_df[[\"listing_id\", \"dayofyear\"]].groupby([\"dayofyear\"]).min()\n",
    "dayofyear_group_by = dayofyear_group_by.add_suffix('_min').reset_index()\n",
    "\n",
    "train_df = train_df.merge(dayofyear_group_by, left_on=['dayofyear'], right_on=['dayofyear'], how='left')\n",
    "train_df['listing_difference'] = train_df['listing_id'] - train_df['listing_id_min']\n",
    "\n",
    "test_df = test_df.merge(dayofyear_group_by, left_on=['dayofyear'], right_on=['dayofyear'], how='left')\n",
    "test_df['listing_difference'] = test_df['listing_id'] - test_df['listing_id_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_to_use' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-9ee14be638b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'manager_level_high'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mfeatures_to_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'manager_level_low'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mfeatures_to_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'manager_level_medium'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mfeatures_to_use\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'manager_level_high'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_to_use' is not defined"
     ]
    }
   ],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import HDFStore\n",
    "\n",
    "store_train_df = HDFStore('train_df.h5')\n",
    "store_test_df = HDFStore('test_df.h5')\n",
    "\n",
    "train_df = store_train_df['train_df']\n",
    "\n",
    "test_df = store_test_df['test_df']\n",
    "\n",
    "store_train_df.close()\n",
    "store_test_df.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 59)\n",
      "(74659, 54)\n",
      "['bathrooms' 'bedrooms' 'building_id' 'created' 'description'\n",
      " 'display_address' 'features' 'interest_level' 'latitude' 'listing_id'\n",
      " 'longitude' 'manager_id' 'photos' 'price' 'street_address'\n",
      " 'description_key_words' 'num_features' 'num_key_words_description'\n",
      " 'description_tokens' 'compound' 'neg' 'neu' 'pos' 'date' 'dayofyear'\n",
      " 'weekofyear' 'weekday' 'hour' 'num_photos' 'price_per_bathroom'\n",
      " 'price_per_bedroom' 'street_address_count' 'neighbourhood' 'price_median'\n",
      " 'price_diff' 'top_10_building' 'top_25_building' 'top_5_building'\n",
      " 'top_50_building' 'top_1_building' 'top_2_building' 'top_15_building'\n",
      " 'top_20_building' 'top_30_building' 'Elevator' 'Dogs Allowed'\n",
      " 'Cats Allowed' 'Hardwood Floors' 'Swimming Pool' 'Doorman'\n",
      " 'Laundry in Unit' 'Fitness center' 'gym' 'Pre-War' 'private-balcony'\n",
      " 'balcony' 'Laundry in Building' 'listing_id_min' 'listing_difference']\n",
      "['bathrooms' 'bedrooms' 'building_id' 'created' 'description'\n",
      " 'display_address' 'features' 'latitude' 'listing_id' 'longitude'\n",
      " 'manager_id' 'photos' 'price' 'street_address' 'description_key_words'\n",
      " 'num_features' 'num_key_words_description' 'description_tokens' 'date'\n",
      " 'dayofyear' 'weekofyear' 'weekday' 'hour' 'num_photos'\n",
      " 'price_per_bathroom' 'price_per_bedroom' 'street_address_count'\n",
      " 'neighbourhood' 'price_median' 'price_diff' 'top_10_building'\n",
      " 'top_25_building' 'top_5_building' 'top_50_building' 'top_1_building'\n",
      " 'top_2_building' 'top_15_building' 'top_20_building' 'top_30_building'\n",
      " 'Elevator' 'Dogs Allowed' 'Cats Allowed' 'Hardwood Floors' 'Swimming Pool'\n",
      " 'Doorman' 'Laundry in Unit' 'Fitness center' 'gym' 'Pre-War'\n",
      " 'private-balcony' 'balcony' 'Laundry in Building' 'listing_id_min'\n",
      " 'listing_difference']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "print(train_df.columns.values)\n",
    "print(test_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_train_df = HDFStore('train_df.h5')\n",
    "store_test_df = HDFStore('test_df.h5')\n",
    "\n",
    "store_train_df['train_df'] = train_df  # save it\n",
    "store_test_df['test_df'] = test_df  #\n",
    "\n",
    "store_train_df.close()\n",
    "store_test_df.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the model building, we will replace all the NaN value and Infinity values since Random Forest is not able to work with missing or infinite values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=train_df.replace(np.nan, 0)\n",
    "test_df=test_df.replace(np.nan, 0)\n",
    "\n",
    "train_df=train_df.replace(np.isinf, 1)\n",
    "test_df=test_df.replace(np.isinf, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Random Forest function to run the model. We will apply the algorithm to the train data set with the pre selected variables and target variable \"interest_level\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform cross validation on the training dataset in order to see when the scores will stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "\n",
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"price_per_bedroom\", \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=500)\n",
    "out_df_1 = pd.DataFrame(preds)\n",
    "out_df_1.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_1[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df_1.to_csv(\"xgb_starter_results_v8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the price_diff which is the most important feature in the result and will run again the Random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"price_per_bedroom\", \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_2 = pd.DataFrame(preds)\n",
    "out_df_2.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_2[\"listing_id\"] = test_df.listing_id.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price_per_bedroom variable ranks the best variable for predicting the interest level so we will remove that one and run again the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_3 = pd.DataFrame(preds)\n",
    "out_df_3.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_3[\"listing_id\"] = test_df.listing_id.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remoce price as a variable since \"price\" is the most important varuable for the lates random forest model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_4 = pd.DataFrame(preds)\n",
    "out_df_4.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_4[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"listing_difference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_5 = pd.DataFrame(preds)\n",
    "out_df_5.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_5[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the \"num_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_6 = pd.DataFrame(preds)\n",
    "out_df_6.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_6[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"num_key_words_description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_7 = pd.DataFrame(preds)\n",
    "out_df_7.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_7[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"latitude\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_8 = pd.DataFrame(preds)\n",
    "out_df_8.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_8[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"longitude\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_9 = pd.DataFrame(preds)\n",
    "out_df_9.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_9[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"manager_level_low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\",\n",
    "                  \"manager_level_medium\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_10 = pd.DataFrame(preds)\n",
    "out_df_10.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_10[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove manager_id_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\", \"manager_level_high\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_11 = pd.DataFrame(preds)\n",
    "out_df_11.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_11[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove \"manager_id_high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"price\", \"price_diff\", \"price_per_bedroom\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                   \"street_address_count\", \"building_id\",\n",
    "                 \"Elevator\", \"Cats Allowed\", \"Dogs Allowed\", 'Hardwood Floors', 'Swimming Pool',\n",
    "                  'Doorman', 'Laundry in Unit', 'Pre-War',\n",
    "                  'Laundry in Building', \"listing_difference\", \"manager_level_low\", \"manager_level_medium\"]\n",
    "\n",
    "\n",
    "train_X = train_df[selected_vars]\n",
    "test_X = test_df[selected_vars]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\n",
    "out_df_12 = pd.DataFrame(preds)\n",
    "out_df_12.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df_12[\"listing_id\"] = test_df.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_df_result = out_df_1.append(out_df_2)\n",
    "out_df_result = out_df_result.append(out_df_3)\n",
    "out_df_result = out_df_result.append(out_df_4)\n",
    "out_df_result = out_df_result.append(out_df_5)\n",
    "out_df_result = out_df_result.append(out_df_6)\n",
    "out_df_result = out_df_result.append(out_df_7)\n",
    "out_df_result = out_df_result.append(out_df_8)\n",
    "out_df_result = out_df_result.append(out_df_9)\n",
    "out_df_result = out_df_result.append(out_df_10)\n",
    "out_df_result = out_df_result.append(out_df_11)\n",
    "out_df_result = out_df_result.append(out_df_12)\n",
    "#out_df_result = out_df_1.append(out_df_2)\n",
    "\n",
    "#print(out_df_1.shape)\n",
    "#print(out_df_2.shape)\n",
    "#print(out_df_result.shape)\n",
    "\n",
    "#print(out_df_result.head())\n",
    "\n",
    "out_df = out_df_result.groupby([\"listing_id\"]).mean()\n",
    "#out_df = out_df.add_suffix('_mean').reset_index()\n",
    "out_df = out_df.reset_index()\n",
    "\n",
    "#print(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df.to_csv(\"xgb_starter_results_v9.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
